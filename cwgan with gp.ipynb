{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.autograd as autograd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Generator(nn.Module):\n",
    "#     def __init__(self, noise_dim, label_dim, output_dim):\n",
    "#         super(Generator, self).__init__()\n",
    "#         self.label_emb = nn.Embedding(label_dim, label_dim)\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(noise_dim + label_dim, 256),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             nn.Linear(256, 512),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             nn.Linear(512, 1024),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             nn.Linear(1024, 2048),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             nn.Linear(2048, output_dim),\n",
    "#             nn.Tanh()\n",
    "#         )\n",
    "#         self.apply(init_weights)\n",
    "\n",
    "#     def forward(self, noise, labels):\n",
    "#         labels = self.label_emb(labels)\n",
    "#         gen_input = torch.cat((noise, labels), dim=1)\n",
    "#         return self.model(gen_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim, label_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.label_dim = label_dim\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(noise_dim + label_dim, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(2048, output_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        labels = torch.nn.functional.one_hot(labels, num_classes=self.label_dim).type(torch.FloatTensor).to(noise.device)\n",
    "        gen_input = torch.cat((noise, labels), dim=1)\n",
    "        return self.model(gen_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Critic(nn.Module):\n",
    "#     def __init__(self, input_dim, label_dim):\n",
    "#         super(Critic, self).__init__()\n",
    "#         self.label_emb = nn.Embedding(label_dim, label_dim)  \n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(input_dim + label_dim, 512),  \n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             nn.Linear(512, 256),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(256, 1)\n",
    "#         )\n",
    "#         self.apply(init_weights)\n",
    "\n",
    "#     def forward(self, inputs, labels):\n",
    "#         labels = self.label_emb(labels).view(labels.size(0), -1)\n",
    "#         disc_input = torch.cat((inputs, labels), dim=1)\n",
    "#         return self.model(disc_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim, label_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.label_dim = label_dim\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim + label_dim, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, inputs, labels):\n",
    "        labels = torch.nn.functional.one_hot(labels, num_classes=self.label_dim).type(torch.FloatTensor).to(inputs.device)\n",
    "        disc_input = torch.cat((inputs, labels), dim=1)\n",
    "        return self.model(disc_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gradient_penalty(C, real_samples, fake_samples, real_labels, lambda_gp):\n",
    "#     \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "#     alpha = torch.rand(real_samples.size(0), 1, device=real_samples.device)\n",
    "#     interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    \n",
    "#     interpolated_labels = real_labels.view(real_labels.size(0), -1)\n",
    "#     d_interpolates = C(interpolates, interpolated_labels)\n",
    "    \n",
    "#     fake = torch.ones(d_interpolates.size(), device=real_samples.device)\n",
    "#     gradients = torch.autograd.grad(\n",
    "#         outputs=d_interpolates, inputs=interpolates,\n",
    "#         grad_outputs=fake, create_graph=True, retain_graph=True, only_inputs=True\n",
    "#     )[0]\n",
    "    \n",
    "#     gradients = gradients.view(gradients.size(0), -1)\n",
    "#     gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * lambda_gp\n",
    "#     return gradient_penalty\n",
    "\n",
    "def gradient_penalty(critic, real_samples, fake_samples, labels, lambda_gp):\n",
    "    alpha = torch.rand(real_samples.size(0), 1).to(real_samples.device)\n",
    "    alpha = alpha.expand_as(real_samples)\n",
    "    \n",
    "    interpolates = alpha * real_samples + ((1 - alpha) * fake_samples)\n",
    "    interpolates.requires_grad_(True)\n",
    "    \n",
    "    d_interpolates = critic(interpolates, labels)\n",
    "    \n",
    "    fake = torch.ones(d_interpolates.size()).to(real_samples.device)\n",
    "    \n",
    "    gradients = autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "    \n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = lambda_gp * ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_csi_samples(generator, label_dim, noise_dim, num_samples, device):\n",
    "#     generator.eval()\n",
    "#     noise = torch.randn(num_samples, noise_dim).to(device)\n",
    "#     labels = torch.randint(0, label_dim, (num_samples,)).to(device)\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         synthetic_data = generator(noise, labels)\n",
    "#     return synthetic_data, labels\n",
    "\n",
    "def generate_csi_samples(generator, label_dim, noise_dim, num_samples, device):\n",
    "    generator.eval()\n",
    "    noise = torch.randn(num_samples, noise_dim).to(device)\n",
    "    labels = torch.randint(0, label_dim, (num_samples,)).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        synthetic_data = generator(noise, labels)  # Adjust labels within the generator call\n",
    "\n",
    "    return synthetic_data, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_gan(data, labels, epochs=50, batch_size=32, noise_dim=100, lr_g=0.00005, lr_c=0.00005, lambda_gp=10):\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     data = data.float()\n",
    "#     labels = labels.long()\n",
    "#     dataset = TensorDataset(data, labels)\n",
    "#     dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#     G = Generator(noise_dim, 8, data.shape[1] * data.shape[2]).to(device)\n",
    "#     C = Critic(data.shape[1] * data.shape[2], 8).to(device)\n",
    "\n",
    "#     optimizer_g = optim.Adam(G.parameters(), lr=lr_g, betas=(0.5, 0.999))\n",
    "#     optimizer_c = optim.Adam(C.parameters(), lr=lr_c, betas=(0.5, 0.999))\n",
    "\n",
    "#     critic_losses = []\n",
    "#     generator_losses = []\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         epoch_critic_loss = 0.0\n",
    "#         epoch_generator_loss = 0.0\n",
    "#         for i, (real_data, real_labels) in enumerate(dataloader):\n",
    "#             real_data = real_data.to(device).view(-1, data.shape[1] * data.shape[2])\n",
    "#             real_labels = real_labels.to(device)\n",
    "\n",
    "#             batch_size = real_data.size(0)\n",
    "#             noise = torch.randn(batch_size, noise_dim).to(device)\n",
    "#             fake_labels = torch.randint(0, 8, (batch_size,)).to(device)\n",
    "#             fake_data = G(noise, fake_labels)\n",
    "\n",
    "#             real_targets = torch.ones(batch_size, 1).to(device)\n",
    "#             fake_targets = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "#             # Train Critic\n",
    "#             optimizer_c.zero_grad()\n",
    "#             real_output = C(real_data, real_labels)\n",
    "#             fake_output = C(fake_data.detach(), fake_labels)\n",
    "#             gp = gradient_penalty(C, real_data, fake_data, real_labels, lambda_gp)\n",
    "#             loss_c = -torch.mean(real_output) + torch.mean(fake_output) + gp\n",
    "#             loss_c.backward(retain_graph=True)\n",
    "#             optimizer_c.step()\n",
    "#             epoch_critic_loss += loss_c.item()\n",
    "\n",
    "#             # Train Generator every 5 iterations of Critic\n",
    "#             if i % 5 == 0:\n",
    "#                 optimizer_g.zero_grad()\n",
    "#                 fake_output = C(fake_data, fake_labels)\n",
    "#                 loss_g = -torch.mean(fake_output)\n",
    "#                 loss_g.backward(retain_graph=True)\n",
    "#                 optimizer_g.step()\n",
    "#                 epoch_generator_loss += loss_g.item()\n",
    "\n",
    "#         critic_losses.append(epoch_critic_loss / len(dataloader))\n",
    "#         generator_losses.append(epoch_generator_loss / (len(dataloader) // 5))\n",
    "        \n",
    "#         # if (epoch + 1) % 500 == 0:\n",
    "#         #     synthetic_csi, _ = generate_csi_samples(G, 8, noise_dim, num_samples=200, device=device)\n",
    "#         #     torch.save(synthetic_csi, f'csi_data_epoch_{epoch + 1}.pth')\n",
    "#         if (epoch + 1) % 20 == 0:\n",
    "#             synthetic_csi, syn_labels = generate_csi_samples(G, 8, noise_dim, 500, device=device)\n",
    "#             torch.save({'synthetic_csi': synthetic_csi, 'labels': syn_labels}, f'csi_data_epoch_{epoch + 1}.pth')\n",
    "\n",
    "#         if (epoch + 1) % 20 == 0:\n",
    "#             print(f'Epoch [{epoch + 1}/{epochs}], Critic Loss: {critic_losses[-1]:.4f}, Generator Loss: {generator_losses[-1]:.4f}')\n",
    "#             torch.save(G.state_dict(), f'generator_{epoch + 1}.pth')\n",
    "\n",
    "#     return G, C, critic_losses, generator_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def create_directory(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(data, labels, epochs=5000, batch_size=32, noise_dim=100, lr_g=0.00005, lr_c=0.00005, lambda_gp=10, save_path='./cwgan_models_exact'):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data = data.float()\n",
    "    labels = labels.long()\n",
    "    dataset = TensorDataset(data, labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    label_dim = 8\n",
    "    G = Generator(noise_dim, label_dim, data.shape[1] * data.shape[2]).to(device)\n",
    "    C = Critic(data.shape[1] * data.shape[2], label_dim).to(device)\n",
    "    \n",
    "    optimizer_g = optim.Adam(G.parameters(), lr=lr_g, betas=(0.5, 0.999))\n",
    "    optimizer_c = optim.Adam(C.parameters(), lr=lr_c, betas=(0.5, 0.999))\n",
    "    \n",
    "    critic_losses = []\n",
    "    generator_losses = []\n",
    "    \n",
    "    create_directory(save_path)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_critic_loss = 0.0\n",
    "        epoch_generator_loss = 0.0\n",
    "        \n",
    "        for i, (real_data, real_labels) in enumerate(dataloader):\n",
    "            real_data = real_data.to(device).view(-1, data.shape[1] * data.shape[2])\n",
    "            real_labels = real_labels.to(device)\n",
    "            batch_size = real_data.size(0)\n",
    "            \n",
    "            noise = torch.randn(batch_size, noise_dim).to(device)\n",
    "            fake_labels = torch.randint(0, label_dim, (batch_size,)).to(device)\n",
    "            fake_data = G(noise, fake_labels)\n",
    "            \n",
    "            real_targets = torch.ones(batch_size, 1).to(device)\n",
    "            fake_targets = torch.zeros(batch_size, 1).to(device)\n",
    "            \n",
    "            # Train Critic\n",
    "            optimizer_c.zero_grad()\n",
    "            real_output = C(real_data, real_labels)\n",
    "            fake_output = C(fake_data.detach(), fake_labels)\n",
    "            gp = gradient_penalty(C, real_data, fake_data, real_labels, lambda_gp)\n",
    "            loss_c = -torch.mean(real_output) + torch.mean(fake_output) + gp\n",
    "            loss_c.backward(retain_graph=True)\n",
    "            optimizer_c.step()\n",
    "            epoch_critic_loss += loss_c.item()\n",
    "            \n",
    "            # Train Generator every 5 iterations of Critic\n",
    "            if i % 5 == 0:\n",
    "                optimizer_g.zero_grad()\n",
    "                fake_output = C(fake_data, fake_labels)\n",
    "                loss_g = -torch.mean(fake_output)\n",
    "                loss_g.backward(retain_graph=True)\n",
    "                optimizer_g.step()\n",
    "                epoch_generator_loss += loss_g.item()\n",
    "        \n",
    "        critic_losses.append(epoch_critic_loss / len(dataloader))\n",
    "        generator_losses.append(epoch_generator_loss / (len(dataloader) // 5))\n",
    "        \n",
    "        if (epoch + 1) % 500 == 0:\n",
    "            synthetic_csi, syn_labels = generate_csi_samples(G, label_dim, noise_dim, 500, device=device)\n",
    "            save_data = {'synthetic_csi': synthetic_csi, 'labels': syn_labels}\n",
    "            torch.save(save_data, os.path.join(save_path, f'csi_data_epoch_{epoch + 1}.pth'))\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Critic Loss: {critic_losses[-1]:.4f}, Generator Loss: {generator_losses[-1]:.4f}')\n",
    "            torch.save(G.state_dict(), os.path.join(save_path, f'generator_{epoch + 1}.pth'))\n",
    "    \n",
    "    return G, C, critic_losses, generator_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dgm82\\AppData\\Local\\Temp\\ipykernel_11844\\1495048577.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data_user1 = torch.load('data_mm_user1.pth')\n",
      "C:\\Users\\dgm82\\AppData\\Local\\Temp\\ipykernel_11844\\1495048577.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  labels_user1 = torch.load('labels_user1.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [500/5000], Critic Loss: -328.9147, Generator Loss: 964.5791\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[88], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m data_user1 \u001b[38;5;241m=\u001b[39m (data_user1 \u001b[38;5;241m-\u001b[39m data_user1\u001b[38;5;241m.\u001b[39mmin()) \u001b[38;5;241m/\u001b[39m (data_user1\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m-\u001b[39m data_user1\u001b[38;5;241m.\u001b[39mmin())  \u001b[38;5;66;03m# Normalize data between 0 and 1\u001b[39;00m\n\u001b[0;32m      5\u001b[0m data_user1 \u001b[38;5;241m=\u001b[39m (data_user1 \u001b[38;5;241m-\u001b[39m data_user1\u001b[38;5;241m.\u001b[39mmean()) \u001b[38;5;241m/\u001b[39m data_user1\u001b[38;5;241m.\u001b[39mstd()  \u001b[38;5;66;03m# Standardize data\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m G, C, critic_losses, generator_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_gan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_user1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_user1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(critic_losses, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCritic Loss\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[87], line 43\u001b[0m, in \u001b[0;36mtrain_gan\u001b[1;34m(data, labels, epochs, batch_size, noise_dim, lr_g, lr_c, lambda_gp, save_path)\u001b[0m\n\u001b[0;32m     41\u001b[0m loss_c \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmean(real_output) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(fake_output) \u001b[38;5;241m+\u001b[39m gp\n\u001b[0;32m     42\u001b[0m loss_c\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 43\u001b[0m \u001b[43moptimizer_c\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m epoch_critic_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_c\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Train Generator every 5 iterations of Critic\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dgm82\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\optim\\optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    482\u001b[0m             )\n\u001b[1;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dgm82\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\optim\\optimizer.py:89\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     88\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 89\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     91\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\dgm82\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\optim\\adam.py:226\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    214\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    216\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    217\u001b[0m         group,\n\u001b[0;32m    218\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    223\u001b[0m         state_steps,\n\u001b[0;32m    224\u001b[0m     )\n\u001b[1;32m--> 226\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\dgm82\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\optim\\optimizer.py:161\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dgm82\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\optim\\adam.py:766\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    763\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    764\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 766\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    767\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    768\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    769\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    772\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    773\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    774\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    775\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    777\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dgm82\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\optim\\adam.py:431\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    429\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 431\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    433\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[0;32m    435\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_user1 = torch.load('data_mm_user1.pth')  \n",
    "labels_user1 = torch.load('labels_user1.pth')  \n",
    "\n",
    "data_user1 = (data_user1 - data_user1.min()) / (data_user1.max() - data_user1.min())  # Normalize data between 0 and 1\n",
    "data_user1 = (data_user1 - data_user1.mean()) / data_user1.std()  # Standardize data\n",
    "\n",
    "G, C, critic_losses, generator_losses = train_gan(data_user1, labels_user1)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(critic_losses, label='Critic Loss', color='blue')\n",
    "plt.plot(generator_losses, label='Generator Loss', color='orange')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Critic Loss and Generator Loss During Training')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gradient_penalty(C, real_data, fake_data, real_labels, lambda_gp):\n",
    "#     batch_size = real_data.size(0)\n",
    "#     epsilon = torch.rand(batch_size, 1, device=real_data.device)\n",
    "#     epsilon = epsilon.expand(real_data.size())\n",
    "#     interpolates = epsilon * real_data + ((1 - epsilon) * fake_data)\n",
    "#     interpolates.requires_grad_(True)\n",
    "\n",
    "#     # Forward pass of interpolates through the critic\n",
    "#     interpolated_labels = real_labels.view(batch_size, -1)\n",
    "#     d_interpolates = C(interpolates, interpolated_labels)\n",
    "\n",
    "#     # Create ones tensor for gradient calculation\n",
    "#     fake = torch.ones(d_interpolates.size(), device=real_data.device)\n",
    "\n",
    "#     # Calculate gradients of interpolates with respect to critic's prediction\n",
    "#     gradients = torch.autograd.grad(\n",
    "#         outputs=d_interpolates, inputs=interpolates,\n",
    "#         grad_outputs=fake, create_graph=True, retain_graph=True, only_inputs=True\n",
    "#     )[0]\n",
    "\n",
    "#     gradients = gradients.view(gradients.size(0), -1)\n",
    "#     gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * lambda_gp\n",
    "#     return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_gan(data, labels, epochs=300, batch_size=32, noise_dim=100, lr_g=1e-4, lr_c=1e-5, lambda_gp=10):\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     data = data.float()\n",
    "#     labels = labels.long()\n",
    "#     dataset = TensorDataset(data, labels)\n",
    "#     dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#     G = Generator(noise_dim, 8, data.shape[1] * data.shape[2]).to(device)\n",
    "#     C = Critic(data.shape[1] * data.shape[2], 8).to(device)\n",
    "\n",
    "#     optimizer_g = optim.Adam(G.parameters(), lr=lr_g, betas=(0.5, 0.999))\n",
    "#     optimizer_c = optim.Adam(C.parameters(), lr=lr_c, betas=(0.5, 0.999))\n",
    "\n",
    "#     critic_losses = []\n",
    "#     generator_losses = []\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         for i, (real_data, real_labels) in enumerate(dataloader):\n",
    "#             real_data = real_data.to(device).view(-1, data.shape[1] * data.shape[2])\n",
    "#             real_labels = real_labels.to(device)\n",
    "\n",
    "#             batch_size = real_data.size(0)\n",
    "#             noise = torch.randn(batch_size, noise_dim).to(device)\n",
    "#             fake_labels = torch.randint(0, 8, (batch_size,)).to(device)\n",
    "#             fake_data = G(noise, fake_labels)\n",
    "\n",
    "#             real_targets = torch.ones(batch_size, 1).to(device)\n",
    "#             fake_targets = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "#             # Train Critic\n",
    "#             optimizer_c.zero_grad()\n",
    "#             real_output = C(real_data, real_labels)\n",
    "#             fake_output = C(fake_data.detach(), fake_labels)\n",
    "#             gp = gradient_penalty(C, real_data, fake_data, real_labels, lambda_gp)\n",
    "#             loss_c = -torch.mean(real_output) + torch.mean(fake_output) + gp\n",
    "#             loss_c.backward(retain_graph=True)\n",
    "#             optimizer_c.step()\n",
    "#             critic_losses.append(loss_c.item())\n",
    "\n",
    "#             # Train Generator every 5 iterations of Critic\n",
    "#             if i % 5 == 0:\n",
    "#                 optimizer_g.zero_grad()\n",
    "#                 fake_output = C(fake_data, fake_labels)\n",
    "#                 loss_g = -torch.mean(fake_output)\n",
    "#                 loss_g.backward(retain_graph=True)\n",
    "#                 optimizer_g.step()\n",
    "#                 generator_losses.append(loss_g.item())\n",
    "\n",
    "#         if (epoch + 1) % 1000 == 0:\n",
    "#             print(f'Epoch [{epoch + 1}/{epochs}], Critic Loss: {loss_c.item():.4f}, Generator Loss: {loss_g.item():.4f}')\n",
    "#             torch.save(G.state_dict(), f'generator_{epoch + 1}.pth')\n",
    "\n",
    "#     return G, C, critic_losses, generator_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load your data and labels from .pth files\n",
    "# data_user1 = torch.load('data_mm_user1.pth')  # replace with the actual path to your .pth file\n",
    "# labels_user1 = torch.load('labels_user1.pth')  # replace with the actual path to your .pth file\n",
    "\n",
    "# G, C, critic_losses, generator_losses = train_gan(data_user1, labels_user1)\n",
    "\n",
    "# # Plotting the losses\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(critic_losses, label='Critic Loss')\n",
    "# plt.plot(generator_losses, label='Generator Loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.title('Critic Loss and Generator Loss During Training')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
